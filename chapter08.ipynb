{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8章：Transformersの高速化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'car_rental', 'score': 0.5490033030509949}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "bert_ckpt = \"transformersbook/bert-base-uncased-finetuned-clinc\"\n",
    "pipe = pipeline(\"text-classification\", model=bert_ckpt)\n",
    "query = \"\"\"Hey, I'd like to rent a vehicle from Nov 1st to Nov 15th in \n",
    "Paris and I need a 15 passenger van\"\"\"\n",
    "pipe(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ベンチマーク用のクラス\n",
    "from datasets import load_metric\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "import numpy as np\n",
    "\n",
    "accuracy_score = load_metric(\"accuracy\")\n",
    "\n",
    "class PerformanceBenchmark:\n",
    "    def __init__(self, pipeline, dataset, optim_type=\"BERT baseline\"):\n",
    "        self.pipeline = pipeline\n",
    "        self.dataset = dataset\n",
    "        self.optim_type = optim_type\n",
    "        \n",
    "    def compute_accuracy(self):\n",
    "        preds, labels = [], []\n",
    "        for example in self.dataset:\n",
    "            pred = self.pipeline(example[\"text\"])[0][\"label\"]\n",
    "            label = example[\"intent\"]\n",
    "            preds.append(intents.str2int(pred))\n",
    "            labels.append(label)\n",
    "        accuracy = accuracy_score.compute(predictions=preds, references=labels)\n",
    "        print(f\"Accuracy on test set - {accuracy['accuracy']:.3f}\")\n",
    "        return accuracy  \n",
    "\n",
    "    def compute_size(self):\n",
    "        state_dict = self.pipeline.model.state_dict()\n",
    "        tmp_path = Path(\"model.pt\")\n",
    "        torch.save(state_dict, tmp_path)\n",
    "        # Calculate size in megabytes\n",
    "        size_mb = Path(tmp_path).stat().st_size / (1024 * 1024)\n",
    "        # Delete temporary file\n",
    "        tmp_path.unlink()\n",
    "        print(f\"Model size (MB) - {size_mb:.2f}\")\n",
    "        return {\"size_mb\": size_mb}\n",
    "    \n",
    "    def time_pipeline(self, query=\"What is the pin number for my account?\"):\n",
    "        latencies = []\n",
    "        # Warmup\n",
    "        for _ in range(10):\n",
    "            _ = self.pipeline(query)\n",
    "        # Timed run\n",
    "        for _ in range(100):\n",
    "            start_time = perf_counter()\n",
    "            _ = self.pipeline(query)\n",
    "            latency = perf_counter() - start_time\n",
    "            latencies.append(latency)\n",
    "        # Compute run statistics\n",
    "        time_avg_ms = 1000 * np.mean(latencies)\n",
    "        time_std_ms = 1000 * np.std(latencies)\n",
    "        print(f\"Average latency (ms) - {time_avg_ms:.2f} +\\- {time_std_ms:.2f}\")\n",
    "        return {\"time_avg_ms\": time_avg_ms, \"time_std_ms\": time_std_ms}\n",
    "\n",
    "    def run_benchmark(self):\n",
    "        metrics = {}\n",
    "        metrics[self.optim_type] = self.compute_size()\n",
    "        metrics[self.optim_type].update(self.time_pipeline())\n",
    "        metrics[self.optim_type].update(self.compute_accuracy())\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset clinc_oos (/home/ace14385kw/.cache/huggingface/datasets/clinc_oos/plus/1.0.0/abcc41d382f8137f039adc747af44714941e8196e845dfbdd8ae7a7e020e6ba1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae9b19b82ef74ad1b2499e545ba3df04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'transfer $100 from my checking to saving account', 'intent': 133}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "clinc = load_dataset(\"clinc_oos\", \"plus\")\n",
    "sample = clinc[\"test\"][42]\n",
    "intents = clinc[\"test\"].features[\"intent\"] # 意図の一覧\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルのチェックポイントを保存\n",
    "import torch\n",
    "\n",
    "torch.save(pipe.model.state_dict(), \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size (MB) - 418.17\n",
      "Average latency (ms) - 32.50 +\\- 18.85\n",
      "Accuracy on test set - 0.867\n"
     ]
    }
   ],
   "source": [
    "pb = PerformanceBenchmark(pipe, clinc[\"test\"])\n",
    "perf_metrics = pb.run_benchmark()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "606baee2f6c6a883ae4e7978cf8ff1aacbfdacdcde4cfb029f60943c1368433c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
