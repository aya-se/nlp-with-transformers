{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8章：Transformersの高速化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'car_rental', 'score': 0.5490033030509949}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "bert_ckpt = \"transformersbook/bert-base-uncased-finetuned-clinc\"\n",
    "pipe = pipeline(\"text-classification\", model=bert_ckpt)\n",
    "query = \"\"\"Hey, I'd like to rent a vehicle from Nov 1st to Nov 15th in \n",
    "Paris and I need a 15 passenger van\"\"\"\n",
    "pipe(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ベンチマーク用のクラス\n",
    "from datasets import load_metric\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "import numpy as np\n",
    "\n",
    "accuracy_score = load_metric(\"accuracy\")\n",
    "\n",
    "class PerformanceBenchmark:\n",
    "    def __init__(self, pipeline, dataset, optim_type=\"BERT baseline\"):\n",
    "        self.pipeline = pipeline\n",
    "        self.dataset = dataset\n",
    "        self.optim_type = optim_type\n",
    "        \n",
    "    def compute_accuracy(self):\n",
    "        preds, labels = [], []\n",
    "        for example in self.dataset:\n",
    "            pred = self.pipeline(example[\"text\"])[0][\"label\"]\n",
    "            label = example[\"intent\"]\n",
    "            preds.append(intents.str2int(pred))\n",
    "            labels.append(label)\n",
    "        accuracy = accuracy_score.compute(predictions=preds, references=labels)\n",
    "        print(f\"Accuracy on test set - {accuracy['accuracy']:.3f}\")\n",
    "        return accuracy  \n",
    "\n",
    "    def compute_size(self):\n",
    "        state_dict = self.pipeline.model.state_dict()\n",
    "        tmp_path = Path(\"model.pt\")\n",
    "        torch.save(state_dict, tmp_path)\n",
    "        # Calculate size in megabytes\n",
    "        size_mb = Path(tmp_path).stat().st_size / (1024 * 1024)\n",
    "        # Delete temporary file\n",
    "        tmp_path.unlink()\n",
    "        print(f\"Model size (MB) - {size_mb:.2f}\")\n",
    "        return {\"size_mb\": size_mb}\n",
    "    \n",
    "    def time_pipeline(self, query=\"What is the pin number for my account?\"):\n",
    "        latencies = []\n",
    "        # Warmup\n",
    "        for _ in range(10):\n",
    "            _ = self.pipeline(query)\n",
    "        # Timed run\n",
    "        for _ in range(100):\n",
    "            start_time = perf_counter()\n",
    "            _ = self.pipeline(query)\n",
    "            latency = perf_counter() - start_time\n",
    "            latencies.append(latency)\n",
    "        # Compute run statistics\n",
    "        time_avg_ms = 1000 * np.mean(latencies)\n",
    "        time_std_ms = 1000 * np.std(latencies)\n",
    "        print(f\"Average latency (ms) - {time_avg_ms:.2f} +\\- {time_std_ms:.2f}\")\n",
    "        return {\"time_avg_ms\": time_avg_ms, \"time_std_ms\": time_std_ms}\n",
    "\n",
    "    def run_benchmark(self):\n",
    "        metrics = {}\n",
    "        metrics[self.optim_type] = self.compute_size()\n",
    "        metrics[self.optim_type].update(self.time_pipeline())\n",
    "        metrics[self.optim_type].update(self.compute_accuracy())\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset clinc_oos (/home/ace14385kw/.cache/huggingface/datasets/clinc_oos/plus/1.0.0/abcc41d382f8137f039adc747af44714941e8196e845dfbdd8ae7a7e020e6ba1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52dabf6306b24d89a16576f3be6d18b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'transfer $100 from my checking to saving account', 'intent': 133}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "clinc = load_dataset(\"clinc_oos\", \"plus\")\n",
    "sample = clinc[\"test\"][42]\n",
    "intents = clinc[\"test\"].features[\"intent\"] # 意図の一覧\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルのチェックポイントを保存\n",
    "import torch\n",
    "\n",
    "torch.save(pipe.model.state_dict(), \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pb = PerformanceBenchmark(pipe, clinc[\"test\"])\n",
    "perf_metrics = pb.run_benchmark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 知識蒸留"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "class DistillationTrainingArguments(TrainingArguments):\n",
    "    def __init__(self, *args, alpha=0.5, temperature=2.0, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import Trainer\n",
    "\n",
    "class DistillationTrainer(Trainer):\n",
    "    def __init__(self, *args, teacher_model=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.teacher_model = teacher_model\n",
    "    \n",
    "    # オーバーライド\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        outputs_stu = model(**inputs) # 生徒モデルの出力\n",
    "        loss_ce = outputs_stu.loss\n",
    "        logits_stu = outputs_stu.logits\n",
    "        with torch.no_grad():\n",
    "            outputs_tea = self.teacher_model(**inputs)\n",
    "            logits_tea = outputs_tea.logits\n",
    "        loss_fct = nn.KLDivLoss(reduction=\"batchmean\") # KL距離\n",
    "        # 入力には対数確率、ラベルには正規確率を想定している点が独特\n",
    "        loss_kd = self.args.temperature ** 2 * loss_fct(\n",
    "            F.log_softmax(logits_stu / self.args.temperature, dim=-1),\n",
    "            F.softmax(logits_tea / self.args.temperature, dim=-1)\n",
    "        )\n",
    "        # 2つの損失を合計して最終的な損失を計算\n",
    "        loss = self.args.alpha * loss_ce + (1 - self.args.alpha) * loss_kd\n",
    "        return (loss, outputs_stu) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "import torch\n",
    "\n",
    "student_ckpt = \"distilbert-base-uncased\"\n",
    "student_tokenizer = AutoTokenizer.from_pretrained(student_ckpt)\n",
    "\n",
    "def tokenize_text(batch):\n",
    "    return student_tokenizer(batch[\"text\"], truncation=True)\n",
    "\n",
    "clinc_enc = clinc.map(tokenize_text, batched=True, remove_columns=[\"text\"]) # \"text\"カラムを削除\n",
    "clinc_enc = clinc_enc.rename_column(\"intent\", \"labels\") # \"intent\"カラムを\"labels\"カラムにリネーム\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    predictions, labels = pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy_score.compute(predictions=predictions, references=labels)\n",
    "\n",
    "batch_size = 48\n",
    "\n",
    "# ファインチューニング後のモデルの保存リポジトリや学習パラメータの指定など\n",
    "finetuned_ckpt = \"distilbert-base-uncased-finetuned-clinc\"\n",
    "student_training_args = DistillationTrainingArguments(\n",
    "    output_dir=finetuned_ckpt, evaluation_strategy = \"epoch\", \n",
    "    num_train_epochs=5, learning_rate=2e-5, \n",
    "    per_device_train_batch_size=batch_size, \n",
    "    per_device_eval_batch_size=batch_size, alpha=1, weight_decay=0.01, \n",
    "    push_to_hub=True)\n",
    "\n",
    "# それぞれのintentとラベルIDのマッピングを生徒モデルに与える\n",
    "    # AutoConfigクラスでカスタムモデルの設定を作成\n",
    "id2label = pipe.model.config.id2label\n",
    "label2id = pipe.model.config.label2id\n",
    "num_labels = intents.num_classes\n",
    "student_config = (AutoConfig.from_pretrained(student_ckpt, num_labels=num_labels, id2label=id2label, label2id=label2id))\n",
    "\n",
    "# GPUの使用\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 生徒モデルの初期化\n",
    "def student_init():\n",
    "    return (AutoModelForSequenceClassification.from_pretrained(student_ckpt, config=student_config).to(device))\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_ckpt = \"transformersbook/bert-base-uncased-finetuned-clinc\"\n",
    "teacher_model = (AutoModelForSequenceClassification\n",
    "                 .from_pretrained(teacher_ckpt, num_labels=num_labels)\n",
    "                 .to(device))\n",
    "distilbert_trainer = DistillationTrainer(model_init=student_init,\n",
    "    teacher_model=teacher_model, args=student_training_args,\n",
    "    train_dataset=clinc_enc['train'], eval_dataset=clinc_enc['validation'],\n",
    "    compute_metrics=compute_metrics, tokenizer=student_tokenizer)\n",
    "\n",
    "distilbert_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert_trainer.push_to_hub(commit_message=\"『機械学習エンジニアのためのTransformers』8章\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_ckpt = \"transformersbook/distilbert-base-uncased-finetuned-clinc\"\n",
    "pipe = pipeline(\"text-classification\", model=finetuned_ckpt)\n",
    "\n",
    "optim_type = \"DistilBERT\"\n",
    "pb = PerformanceBenchmark(pipe, clinc[\"test\"], optim_type=optim_type)\n",
    "perf_metrics.update(pb.run_benchmark())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metrics(perf_metrics, current_optim_type):\n",
    "    df = pd.DataFrame.from_dict(perf_metrics, orient='index')\n",
    "\n",
    "    for idx in df.index:\n",
    "        df_opt = df.loc[idx]\n",
    "        # Add a dashed circle around the current optimization type\n",
    "        if idx == current_optim_type:\n",
    "            plt.scatter(df_opt[\"time_avg_ms\"], df_opt[\"accuracy\"] * 100, \n",
    "                        alpha=0.5, s=df_opt[\"size_mb\"], label=idx, \n",
    "                        marker='$\\u25CC$')\n",
    "        else:\n",
    "            plt.scatter(df_opt[\"time_avg_ms\"], df_opt[\"accuracy\"] * 100, \n",
    "                        s=df_opt[\"size_mb\"], label=idx, alpha=0.5)\n",
    "            \n",
    "    legend = plt.legend(bbox_to_anchor=(1,1))\n",
    "    for handle in legend.legendHandles:\n",
    "        handle.set_sizes([20])\n",
    "\n",
    "    plt.ylim(80,90)\n",
    "    # Use the slowest model to define the x-axis range\n",
    "    xlim = int(perf_metrics[\"BERT baseline\"][\"time_avg_ms\"] + 3)\n",
    "    plt.xlim(1, xlim)\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.xlabel(\"Average latency (ms)\")\n",
    "    plt.show()\n",
    "    \n",
    "plot_metrics(perf_metrics, optim_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 量子化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.quantization import quantize_dynamic\n",
    "\n",
    "model_ckpt = \"transformersbook/distilbert-base-uncased-distilled-clinc\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = (AutoModelForSequenceClassification.from_pretrained(model_ckpt).to(\"cpu\"))\n",
    "\n",
    "model_quantized = quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8) # 動的量子化の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONNXとONNX Runtimeを使った推論の最適化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenMPの環境変数の設定\n",
    "import os\n",
    "from psutil import cpu_count\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = f\"{cpu_count()}\"\n",
    "os.environ[\"OMP_WAIT_POLICY\"] = \"ACTIVE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.convert_graph_to_onnx import convert\n",
    "\n",
    "model_ckpt = \"transformersbook/distilbert-base-uncased-distilled-clinc\"\n",
    "onnx_model_path = Path(\"onnx/model.onnx\")\n",
    "convert(framework=\"pt\", model=model_ckpt, tokenizer=tokenizer, \n",
    "        output=onnx_model_path, opset=12, pipeline_name=\"text-classification\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "606baee2f6c6a883ae4e7978cf8ff1aacbfdacdcde4cfb029f60943c1368433c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
